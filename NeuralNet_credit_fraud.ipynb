{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas for Fraud check : \n",
    "# 1) Check locations of transaction - 2 different transactions in a short period. \n",
    "# 2) User's Credit card use history \n",
    "# 3) Classifcation on different frauds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_Autoencoder_Fraud_data.ipynb\n",
      "Imbalanced_data_credit_card.ipynb\n",
      "Imballaned_data_undersamling.pdf\n",
      "Untitled.ipynb\n",
      "Untitled1.ipynb\n",
      "creditcard.csv\n",
      "logs\n",
      "model.h5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../CREDIT_FRAUD\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.165980e-15</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>-1.373150e-15</td>\n",
       "      <td>2.086869e-15</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.490107e-15</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.177556e-16</td>\n",
       "      <td>-2.406455e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.656562e-16</td>\n",
       "      <td>-3.444850e-16</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>4.471968e-15</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>1.687098e-15</td>\n",
       "      <td>-3.666453e-16</td>\n",
       "      <td>-1.220404e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.165980e-15  3.416908e-16 -1.373150e-15  2.086869e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.490107e-15 -5.556467e-16  1.177556e-16 -2.406455e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.656562e-16 -3.444850e-16  2.578648e-16  4.471968e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.340915e-16  1.687098e-15 -3.666453e-16 -1.220404e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data set \n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "# Exploring the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irinamahmudjanova/anaconda3/envs/dlaiwtpip/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Creating Train Set, Dev Set & Train set\n",
    "\n",
    "# Converting the csv data into matrix \n",
    "columns = \"Time V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 Amount\".split()\n",
    "X = pd.DataFrame.as_matrix(df,columns=columns)\n",
    "Y = df.Class\n",
    "Y = Y.values.reshape(Y.shape[0],1)\n",
    "X.shape\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.06)\n",
    "X_test, X_dev, Y_test, Y_dev = train_test_split(X_test,Y_test, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   402,   1681,   3062,   4727,   4867,   5167,   7091,   7526,\n",
       "          7799,   7872,   7879,   8655,   9115,  11426,  11544,  11934,\n",
       "         13191,  13214,  13726,  14479,  15360,  15798,  16585,  16888,\n",
       "         17468,  18050,  18442,  20023,  20059,  20768,  21321,  21469,\n",
       "         21615,  24304,  24953,  24973,  25337,  25389,  26460,  26487,\n",
       "         26835,  28466,  29528,  29759,  30205,  31560,  32088,  32518,\n",
       "         32912,  33357,  33870,  36566,  38432,  38634,  38783,  39058,\n",
       "         39560,  40349,  40363,  40379,  42394,  44530,  44640,  44963,\n",
       "         45855,  46171,  46604,  46718,  47487,  47593,  47723,  47782,\n",
       "         49319,  50134,  50398,  51459,  51816,  52563,  52678,  53989,\n",
       "         54138,  54395,  54438,  54665,  55190,  56059,  56065,  56861,\n",
       "         57066,  57091,  57738,  58413,  59079,  59676,  60240,  60307,\n",
       "         60623,  60970,  62097,  62520,  62857,  62944,  63289,  65139,\n",
       "         66341,  66516,  66529,  66550,  68050,  68335,  68738,  69249,\n",
       "         70153,  70390,  70563,  70854,  71524,  72035,  72652,  72813,\n",
       "         72855,  73959,  75865,  76818,  77047,  77581,  77868,  78392,\n",
       "         78878,  79326,  79681,  79930,  80842,  82539,  84187,  84398,\n",
       "         84602,  85315,  86496,  86661,  87451,  87857,  88317,  88392,\n",
       "         89158,  89446,  89789,  90262,  91755,  92086,  92198,  92929,\n",
       "         93579,  93929,  95405,  96160,  98513,  99728,  99977, 100271,\n",
       "        100359, 100571, 101928, 102207, 103001, 103729, 104008, 104251,\n",
       "        104864, 105334, 106250, 106820, 108057, 109080, 109387, 109557,\n",
       "        109782, 109908, 110012, 110238, 110829, 111055, 112794, 112880,\n",
       "        112944, 113515, 115209, 115414, 117176, 117750, 118583, 118958,\n",
       "        119100, 120177, 120834, 120966, 121380, 121497, 121509, 121665,\n",
       "        124486, 124882, 125246, 125444, 125507, 126038, 126340, 126815,\n",
       "        128124, 128782, 129671, 130136, 130153, 130478, 130801, 131398,\n",
       "        131726, 131970, 133424, 134767, 134795, 135484, 136847, 136915,\n",
       "        137823, 137897, 138327, 138339, 138549, 139638, 139932, 140062,\n",
       "        140770, 140908, 141245, 141596, 142215, 143048, 144467, 147932,\n",
       "        149489, 151264, 151433, 152020, 152892, 153068, 155037, 155722,\n",
       "        156009, 156174, 156302, 156798, 157062, 157477, 158133, 158782,\n",
       "        158877, 159033, 160080, 160899, 161385, 161866, 162492, 162541,\n",
       "        162607, 162793, 164045, 164397, 164453, 165332, 166003, 166531,\n",
       "        167264, 167420, 167977, 168281, 168816, 169091, 169504, 169704,\n",
       "        170188, 170872, 171597, 173207, 173263, 173378, 173652, 174741,\n",
       "        175412, 175491, 176404, 177024, 177118, 177229, 178988, 180097,\n",
       "        180177, 180579, 181220, 181688, 182151, 182300, 183564, 184196,\n",
       "        185247, 185432, 185528, 185864, 186225, 188156, 188899, 188975,\n",
       "        189038, 191166, 192290, 192891, 193283, 193315, 193409, 195232,\n",
       "        195454, 196705, 197057, 197066, 198711, 198806, 199468, 200127,\n",
       "        200203, 200986, 201355, 201555, 201730, 201776, 204318, 204487,\n",
       "        204938, 205514, 206641, 207115, 208002, 209203, 210323, 211047,\n",
       "        211697, 211774, 211844, 212243, 212327, 213035, 213132, 214464,\n",
       "        215806, 215812, 215871, 216964, 217657, 217694, 217740, 218339,\n",
       "        219181, 219225, 219886, 220459, 220586, 220593, 220726, 221757,\n",
       "        222114, 223806, 224110, 224483, 224882, 225368, 225890, 226227,\n",
       "        226375, 226618, 226641, 227451, 228452, 228621, 228952, 229933,\n",
       "        230459, 230819, 231485, 232766, 232769, 233049, 233546, 233946,\n",
       "        234391, 235490, 236266, 237131, 237306, 237784, 238364, 238375,\n",
       "        238691, 239640, 239873, 240184, 240241, 241408, 242757, 242908,\n",
       "        244134, 244137, 244158, 245217, 246390, 247839, 248356, 248404,\n",
       "        249862, 250032, 250300, 251904, 253180, 253240, 253504, 253972,\n",
       "        254182, 254682, 254720, 255086, 255285, 255332, 255517, 255981,\n",
       "        256253, 256508, 256585, 256696, 256926, 257780, 258060, 258362,\n",
       "        258488, 258791, 259529, 259620, 259675, 260010, 261014, 261260,\n",
       "        261264, 262272, 262719, 263446, 263481, 263793, 264257, 264541,\n",
       "        265275, 265469, 265497, 266621, 267128, 267550]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there is Classification Values - 0/1 in training set and other set \n",
    "\n",
    "np.where(Y_train == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 471,  632,  704, 1099, 1614, 1984, 2567, 3334, 3446, 3881, 5750,\n",
       "        6043, 6924, 7955]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(Y_test == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1147, 1386, 1726, 2261, 3438, 4116, 5231, 5523, 6307, 6541, 7278,\n",
       "        7588, 8054, 8099, 8346, 8502]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(Y_dev == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of training Examples : 267718\n",
      "No of test Examples : 8544\n",
      "No of dev Examples : 8545\n",
      "Shape of training data : (267718, 30)\n",
      "Shape of test data : (8544, 30)\n",
      "Shape of dev data : (8545, 30)\n",
      "Shape of Y test data : (8544, 1)\n",
      "Shape of Y dev data : (8545, 1)\n"
     ]
    }
   ],
   "source": [
    "# Checking the shape's of the new data set as matrix \n",
    "print(\"No of training Examples : \"+str(X_train.shape[0]))  # 94% data \n",
    "print(\"No of test Examples : \"+str(X_test.shape[0]))       # 3% data\n",
    "print(\"No of dev Examples : \"+str(X_dev.shape[0]))         # 3% data\n",
    "\n",
    "print(\"Shape of training data : \"+str(X_train.shape))\n",
    "print(\"Shape of test data : \"+str(X_test.shape))\n",
    "print(\"Shape of dev data : \"+str(X_dev.shape))\n",
    "print(\"Shape of Y test data : \"+str(Y_test.shape))\n",
    "print(\"Shape of Y dev data : \"+str(Y_dev.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of training Examples : (30, 267718)\n",
      "No of test Examples : (1, 267718)\n",
      "No of X_dev Examples : (30, 8545)\n",
      "No of Y_dev test Examples : (1, 8545)\n",
      "No of X_test Examples : (30, 8544)\n",
      "No of Y_test Examples : (1, 8544)\n",
      "No of Sanity_test : [ 1.41662000e+05  1.94420999e+00 -1.03558044e-01 -2.05069118e+00\n",
      "  1.23676947e+00]\n"
     ]
    }
   ],
   "source": [
    "#Flatten the data to so that all Features/X Variables \n",
    "X_train_flatten = X_train.reshape(X_train.shape[0],-1).T\n",
    "Y_train_flatten = Y_train.reshape(Y_train.shape[0],-1).T\n",
    "X_dev_flatten = X_dev.reshape(X_dev.shape[0],-1).T\n",
    "Y_dev_flatten = Y_dev.reshape(Y_dev.shape[0],-1).T\n",
    "X_test_flatten = X_test.reshape(X_test.shape[0],-1).T\n",
    "Y_test_flatten = Y_test.reshape(Y_test.shape[0],-1).T\n",
    "\n",
    "print(\"No of training Examples : \"+str(X_train_flatten.shape))  \n",
    "print(\"No of test Examples : \"+str(Y_train_flatten.shape))  \n",
    "print(\"No of X_dev Examples : \"+str(X_dev_flatten.shape))  \n",
    "print(\"No of Y_dev test Examples : \"+str(Y_dev_flatten.shape))  \n",
    "print(\"No of X_test Examples : \"+str(X_test_flatten.shape))  \n",
    "print(\"No of Y_test Examples : \"+str(Y_test_flatten.shape))\n",
    "print(\"No of Sanity_test : \"+str(X_train_flatten[0:5,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of X_train_set shape : (30, 267718)\n",
      "No of Y_train_set shape : (1, 267718)\n"
     ]
    }
   ],
   "source": [
    "# Normalize features and create final Train set \n",
    "X_train_set = preprocessing.normalize(X_train_flatten)\n",
    "Y_train_set = Y_train_flatten\n",
    "\n",
    "print(\"No of X_train_set shape : \"+str(X_train_set.shape))  \n",
    "print(\"No of Y_train_set shape : \"+str(Y_train_set.shape)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcation to intialize weights for forward propogration \n",
    "def intialize_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1,L):\n",
    "        parameters['W'+str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b'+str(l)] = np.zeros((layer_dims[l],1))\n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 =[[ 1.36438878e-02 -3.41601892e-03 -9.04576861e-03 -1.34184950e-02\n",
      "  -1.97793247e-03 -1.37955863e-02  2.23350120e-04  5.35663873e-03\n",
      "   2.07226739e-02 -2.35604416e-02 -1.00515021e-02  7.34222206e-04\n",
      "  -4.55032766e-03  4.48403122e-03  9.83478802e-03 -1.92296846e-02\n",
      "  -1.00389715e-02  5.15746784e-03 -1.04390438e-02  9.02118696e-03\n",
      "  -1.46619610e-02  2.26016602e-03  2.74398673e-03  7.10339895e-03\n",
      "   1.42352626e-03 -7.65940290e-03 -1.29988716e-02  3.33826261e-03\n",
      "  -4.75241906e-04  1.75595838e-03]\n",
      " [ 9.77972392e-03 -8.39853961e-03 -1.02064836e-02 -4.63784471e-03\n",
      "   5.27715408e-03 -6.81638763e-03  5.49097046e-03 -9.23598757e-03\n",
      "  -3.40652886e-03 -6.07621296e-03  1.66959725e-02  1.05319729e-02\n",
      "  -4.35230137e-03  2.23307686e-02  5.31709685e-03 -2.77466326e-03\n",
      "  -1.95787912e-03  1.13633071e-02  4.71789788e-03 -1.45198263e-02\n",
      "  -1.65870951e-02 -1.29472802e-03 -3.50731334e-03 -1.27725220e-02\n",
      "  -1.16151492e-02 -1.00981187e-02  1.59486416e-02  4.26837409e-03\n",
      "  -1.01681707e-02 -4.91880463e-03]\n",
      " [-3.63006827e-03 -1.07701705e-02 -4.07590667e-03 -2.44618769e-03\n",
      "  -1.72279950e-02 -9.84558524e-03 -1.02561426e-02 -8.98936127e-03\n",
      "  -1.38600109e-02  1.93971513e-03 -9.76988524e-03  7.57825352e-03\n",
      "   8.11588431e-03  7.62221291e-03  4.24594041e-04 -1.03669086e-02\n",
      "   1.15919913e-02 -1.21681279e-02  1.03594006e-02  7.39264733e-03\n",
      "  -5.60718415e-03  5.65075883e-03 -8.10082926e-03 -5.61764012e-03\n",
      "   6.77622857e-03  2.66215208e-03 -7.81820490e-03  1.04805985e-03\n",
      "  -5.70203530e-04  1.12936018e-02]\n",
      " [-2.89664889e-03 -1.87313071e-02  5.83587085e-03 -5.11115087e-03\n",
      "   7.86765113e-04 -1.97898349e-02 -3.47872966e-03  1.43471708e-02\n",
      "  -1.51597881e-02  1.25712030e-02 -2.75138214e-03  4.55317643e-04\n",
      "   3.23042664e-03  1.37261462e-02 -1.12671713e-02 -9.33300353e-03\n",
      "   6.78908897e-03 -9.62775932e-04  1.54444533e-02  3.33727450e-03\n",
      "   1.09612384e-02  3.01327873e-03  1.38132443e-02  5.84689300e-04\n",
      "   4.34942782e-03  8.83388369e-03  1.31440357e-02 -7.18516121e-03\n",
      "   3.80827312e-03 -7.63516757e-03]\n",
      " [ 2.26513381e-02  6.39380738e-03 -5.31148232e-03 -1.47462300e-02\n",
      "  -1.00135002e-02 -7.22576410e-03  4.49888578e-03 -4.73681686e-03\n",
      "   3.32065482e-04 -2.85279282e-02  7.79153447e-03  2.37110891e-02\n",
      "  -1.39461596e-02 -1.38531521e-02  6.07207695e-03 -1.51481963e-02\n",
      "  -1.03062583e-02 -4.12572859e-03 -1.91370282e-03  8.50943258e-03\n",
      "   1.16321772e-02  8.10435891e-03  1.41130114e-03 -6.98738784e-03\n",
      "   7.10987728e-03 -1.84780141e-02 -2.74820374e-02 -1.44059306e-02\n",
      "   9.67954308e-04 -1.94171100e-02]\n",
      " [ 1.03730894e-02 -5.72940429e-03 -1.76419362e-02  7.01439359e-03\n",
      "   5.80756302e-03 -6.12582547e-03  1.72621193e-03 -6.31596346e-04\n",
      "  -1.31710829e-02 -1.73955154e-03 -5.49399104e-03  5.67446325e-03\n",
      "   9.75517304e-03  2.42260573e-03  1.93647871e-03  9.86202487e-04\n",
      "   8.40578429e-04 -3.56391968e-03 -4.71711954e-03  9.68839341e-03\n",
      "   7.89914527e-03  5.37868592e-03 -3.47948949e-03 -4.50570437e-03\n",
      "  -2.82572480e-02 -5.27842595e-03  4.52003084e-03  1.90865487e-02\n",
      "  -4.30061595e-03  1.99771702e-05]\n",
      " [-1.23387201e-02 -6.28123349e-03  2.56812080e-05  1.02201873e-02\n",
      "  -2.48265332e-03  5.83944352e-03 -6.50371901e-03  1.00419045e-02\n",
      "   1.05462133e-02 -1.34608920e-02  6.89775846e-05  3.30086206e-03\n",
      "   2.05283454e-02 -8.59250092e-03  7.59791078e-03 -7.12134235e-03\n",
      "   9.31552969e-03 -1.11173954e-02 -1.40766986e-02 -1.23255150e-02\n",
      "  -6.43188191e-03 -8.03092185e-03  7.42706572e-03 -9.83713004e-03\n",
      "  -1.27912015e-04 -3.13470400e-03  9.49662549e-03 -1.29238576e-02\n",
      "   1.52878261e-04  4.18635494e-03]\n",
      " [-1.16414154e-03  3.11058804e-03  9.53623219e-03  6.71356052e-03\n",
      "   9.32722259e-03 -9.01383210e-04 -1.25980323e-02 -7.17996985e-03\n",
      "   1.68308427e-02 -6.83556331e-03 -1.26832330e-02 -3.83128994e-03\n",
      "  -1.14149087e-03 -1.90742219e-03  3.84831099e-03  1.08865333e-02\n",
      "  -2.13179995e-03  1.62306947e-02 -1.23420145e-02 -1.73905656e-02\n",
      "  -3.68317032e-03 -1.01098403e-02  1.07653420e-02 -4.79527561e-03\n",
      "  -6.88009576e-03  5.13299930e-03  2.39832603e-02  1.14145747e-02\n",
      "  -1.00830260e-02  1.39335231e-02]\n",
      " [-1.46051919e-03  1.03651724e-02  1.23955690e-02  5.18480820e-04\n",
      "   2.35309462e-03 -5.26276069e-03 -4.43622256e-03  2.41213115e-03\n",
      "  -2.35418896e-03 -6.24335084e-03 -1.07896776e-03  6.38793634e-03\n",
      "   6.92081331e-03  1.06904889e-02 -9.15295852e-03  8.65333131e-03\n",
      "   2.57895452e-02  1.29281071e-02 -1.32574096e-02  9.45679206e-04\n",
      "   1.26903647e-03  1.26507414e-03  1.43957389e-03 -4.38833206e-03\n",
      "  -1.95892408e-03  5.58689908e-03  8.29564649e-03  6.54277610e-03\n",
      "  -3.60909006e-03  4.17265525e-03]\n",
      " [-1.44187660e-02  7.59709035e-03  1.13559537e-03 -5.57524155e-03\n",
      "  -7.54437563e-03  1.04262863e-02  6.30319864e-03  9.50727028e-03\n",
      "  -1.69410416e-02 -7.19786828e-04 -6.79834300e-03 -3.58007269e-03\n",
      "  -1.11486503e-02  9.57605788e-03  6.63494004e-03 -5.76934788e-05\n",
      "  -2.20639077e-03 -7.95610694e-03  1.39542057e-02  7.68396840e-03\n",
      "   1.14047063e-02 -5.89061541e-03  4.40583935e-03  1.25108348e-03\n",
      "  -1.07926752e-02  4.26770948e-03 -2.42145543e-03 -1.40583522e-02\n",
      "  -1.44332023e-02  7.12709359e-03]\n",
      " [ 1.99818693e-02 -4.52861544e-03 -5.89594055e-03 -1.90947078e-03\n",
      "   1.26737651e-02  5.36757062e-03  3.97138559e-03 -1.82424484e-02\n",
      "  -9.85241234e-03  7.55330413e-03 -4.59202922e-03 -3.80063036e-03\n",
      "  -4.51187533e-03  7.05728668e-04  1.50248482e-02  5.33006428e-03\n",
      "   1.48320485e-02  9.67057571e-03  4.97254855e-03 -2.99772321e-03\n",
      "  -1.16922586e-02  1.01349378e-03  7.31920698e-04  1.19692053e-02\n",
      "  -2.03512540e-02  1.38358837e-02 -7.17152202e-03  8.65346602e-04\n",
      "  -1.78710038e-04 -3.90202461e-03]\n",
      " [-1.04665543e-03 -6.04112293e-03 -6.64363505e-03 -8.84004171e-03\n",
      "   7.24622280e-03  8.88183419e-03  5.25419038e-05  9.46340091e-03\n",
      "  -1.06712653e-02 -4.34804796e-03  4.26353181e-03 -5.67033179e-03\n",
      "  -9.43392323e-04  1.24994431e-02  2.62577940e-03  8.10409032e-03\n",
      "  -1.43425858e-02 -4.48864869e-03  4.54151538e-03  4.01476880e-03\n",
      "   2.62542379e-02 -1.09302059e-02  1.45179996e-02 -1.42596179e-02\n",
      "  -1.24680215e-02 -7.12021783e-03  3.38335829e-03 -9.00837754e-03\n",
      "  -6.01300380e-03  2.32508924e-04]\n",
      " [ 9.94036309e-03  1.86603525e-02  1.79465063e-02  1.35121600e-03\n",
      "  -6.08804432e-04 -7.77998176e-03 -1.79580221e-02 -5.32922756e-03\n",
      "  -8.39621736e-03 -1.30215531e-02  3.28231608e-03 -5.34564279e-03\n",
      "   5.62556761e-05  1.51616236e-02 -9.99958386e-03  1.89779302e-02\n",
      "   8.59860108e-03 -6.75232671e-04 -2.54197536e-04  2.26792501e-03\n",
      "  -2.66596140e-03 -5.37266387e-03  4.19933676e-03  4.76606384e-03\n",
      "   5.29100702e-03  4.83645378e-03 -5.64190146e-03 -3.02492741e-03\n",
      "  -2.40277188e-02 -1.69028701e-02]\n",
      " [-5.78854285e-03  5.85789574e-03 -2.20999023e-03 -4.92684185e-04\n",
      "  -8.23152839e-03 -1.09620911e-02 -7.31354596e-03 -3.52326508e-03\n",
      "  -8.21421279e-04 -3.39714325e-03 -1.91301277e-02 -1.39234150e-02\n",
      "   1.02699962e-02  6.21017708e-03 -5.29239172e-03  6.12589211e-03\n",
      "   6.07163335e-03 -6.54569923e-03  1.98042356e-02  2.11878569e-03\n",
      "  -8.61897484e-03  2.30883266e-03 -1.32879944e-02 -5.76548597e-03\n",
      "  -3.05952409e-03 -8.95761847e-03  5.15644284e-04 -6.94193049e-03\n",
      "   1.54368116e-02 -1.32327822e-02]\n",
      " [-4.55676026e-03 -6.85100467e-03 -1.05263694e-02 -1.50229010e-04\n",
      "  -9.45438510e-03  5.89922380e-03  1.55487764e-02  1.00372458e-02\n",
      "  -2.77938631e-02  9.95713092e-03 -3.09233221e-04 -3.65036291e-03\n",
      "   1.41599679e-02  2.36117820e-02 -3.61258071e-03  6.82950460e-03\n",
      "   9.88541325e-03 -4.01677439e-03  4.08461032e-04  3.82150392e-03\n",
      "  -7.46669351e-03 -2.29407626e-02 -7.39670783e-03  2.15636709e-02\n",
      "   1.48105284e-03 -2.96349991e-03  3.21728728e-03 -1.57099686e-02\n",
      "   8.05065054e-04 -1.54336856e-02]\n",
      " [ 5.81088696e-04  9.65339890e-03  3.17793069e-03  3.86844379e-04\n",
      "  -1.53831028e-03 -2.01777411e-03  6.82696173e-03  2.50699721e-02\n",
      "   2.83236475e-03  2.43107924e-03  1.20380921e-02 -1.93461797e-02\n",
      "   4.97271026e-03 -9.78359506e-03  2.04634061e-03 -8.13821775e-03\n",
      "  -1.37844096e-02 -8.38651022e-03  7.27500277e-03 -2.26418444e-03\n",
      "   1.78648416e-02 -1.09908144e-03  1.12187845e-03 -4.11816512e-04\n",
      "   4.38562710e-03 -9.39728918e-03  3.18896752e-03  6.90201989e-03\n",
      "   7.32322450e-03  4.23309746e-03]\n",
      " [ 4.25771309e-03 -1.31387513e-04  9.24847934e-03 -1.13781703e-02\n",
      "  -1.10053068e-02  7.26679553e-03 -7.74855423e-03 -3.58479566e-03\n",
      "   1.67124014e-02  6.50875941e-04  8.09014020e-03 -9.58571190e-03\n",
      "  -2.49218352e-03 -7.97745097e-03  3.11186872e-03  3.81479572e-03\n",
      "  -1.14703253e-02 -9.22176131e-03  6.67512676e-03 -5.88562044e-03\n",
      "   7.26184815e-03 -4.64122111e-03 -1.26752480e-02  2.42683104e-02\n",
      "  -8.37807541e-03 -1.64928125e-02 -6.24752874e-03  6.77380616e-03\n",
      "   5.67907096e-03  9.50203685e-04]\n",
      " [-6.30381570e-04 -1.87445336e-03 -1.06690974e-02  1.16594073e-02\n",
      "  -3.47278959e-03  1.85679459e-02 -4.69396709e-03  3.14100854e-03\n",
      "  -5.44990825e-03  4.29217755e-03  5.15704287e-03 -8.14722201e-04\n",
      "  -1.04035626e-03  7.84422039e-03 -7.27138975e-03 -3.93949428e-03\n",
      "   1.00165921e-03  1.30563143e-02  3.48808459e-04 -1.05152405e-02\n",
      "  -4.60159723e-03  2.51200125e-02 -2.46014726e-02 -1.27746081e-02\n",
      "  -7.27018327e-03 -1.41195583e-02 -1.19193761e-02 -4.57491228e-03\n",
      "   6.46413130e-03  1.96127808e-03]\n",
      " [ 4.50121947e-03  1.08641534e-02  5.45660498e-03  1.77877442e-02\n",
      "   1.79233040e-02 -2.91201463e-03  8.98244279e-03 -5.39561544e-03\n",
      "   1.01533971e-02  2.70221665e-03  9.75970662e-05 -4.08970732e-03\n",
      "  -1.09689822e-02 -1.29473288e-02 -2.44274995e-03  5.33268508e-03\n",
      "   1.99910603e-02  5.04115502e-04 -3.50745235e-03  1.43483766e-02\n",
      "   9.09267222e-03 -6.65103332e-03  6.25122778e-03 -7.06824992e-03\n",
      "  -6.53800128e-03  2.14131824e-02  9.38648945e-03 -4.43044890e-04\n",
      "   4.49292361e-03 -1.71134384e-02]\n",
      " [ 2.13587661e-02  1.58341582e-02  4.60086155e-03 -2.80551657e-03\n",
      "   6.33201538e-04  2.54797586e-03  9.18035804e-03  3.21368899e-03\n",
      "   2.62946066e-02  6.31175151e-03  2.37724103e-02 -1.39838707e-03\n",
      "  -6.83563525e-03 -7.95005766e-03 -1.10778198e-02  1.30265205e-02\n",
      "  -1.25437047e-02  2.47354351e-03  3.77771458e-04  7.44772359e-04\n",
      "   1.56479483e-02 -5.68840946e-03 -1.37552259e-02  1.92205957e-02\n",
      "  -3.98363104e-03 -5.21473099e-03  1.24923644e-02  4.94104792e-03\n",
      "  -2.44174277e-02 -1.66824133e-03]]\n",
      "b1 =[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 =[[-2.02445698e-03 -1.77332304e-02 -6.53702603e-03  1.69655809e-02\n",
      "  -1.51736121e-02  5.75703886e-03  9.47948165e-03  9.73119899e-03\n",
      "   2.00821355e-02 -3.17001621e-03 -4.82356457e-03 -5.66990195e-03\n",
      "   3.78648851e-03 -3.24440005e-03 -3.19524178e-03 -3.80706603e-03\n",
      "  -6.05927966e-05 -1.20342887e-02  3.57774945e-04 -6.27210395e-03]\n",
      " [ 1.11745451e-02 -2.44909076e-02 -1.24545032e-02  1.18542252e-02\n",
      "   1.49043052e-02  1.09642073e-02 -8.52173034e-03  2.90059972e-03\n",
      "  -7.92615819e-03 -1.22107570e-02 -1.87902225e-03 -2.25479108e-02\n",
      "   1.45673634e-02 -1.27865575e-02 -6.14611944e-03  5.16181091e-03\n",
      "   8.79826047e-04  1.17470876e-02  9.80094245e-04 -1.02409861e-02]\n",
      " [-1.26583181e-02  1.01949653e-02  2.01630887e-02 -1.58053165e-02\n",
      "  -1.49634731e-02 -5.38969064e-03  1.38261818e-02 -1.43139506e-02\n",
      "  -2.35560224e-03 -7.85024243e-03  4.26963410e-03 -1.94319312e-04\n",
      "  -1.32365919e-03  8.32899337e-03  9.06826036e-03  1.53175948e-02\n",
      "  -3.97307061e-03  5.69151260e-03  7.63889098e-03  5.47717719e-04]\n",
      " [ 1.14824507e-02  2.71984301e-03 -7.60155117e-03  2.23093220e-02\n",
      "  -1.73628891e-02 -1.66425441e-03  1.11020921e-02  9.39012629e-03\n",
      "   1.74276366e-02  6.31227415e-03 -4.85523585e-03  5.48510138e-04\n",
      "  -1.71905070e-02 -4.54286725e-03  4.94329827e-04 -3.40158524e-03\n",
      "  -7.49857143e-03  2.60064737e-03  5.93593434e-03  2.81172579e-03]\n",
      " [ 2.12550842e-02  6.03295485e-03 -1.38651821e-02  6.78629451e-03\n",
      "  -2.67772493e-03  2.24619383e-02  2.51852279e-03  6.56064413e-03\n",
      "   7.59368507e-03  7.29662246e-04 -1.37860299e-02 -5.47158295e-03\n",
      "   7.48951869e-04  6.61280932e-03  8.21106678e-03  1.88939599e-02\n",
      "   2.10016465e-02  1.31306529e-02  1.49415003e-02  2.71687456e-02]\n",
      " [-2.59781111e-03  8.53325180e-03  8.69240124e-04 -1.50939105e-02\n",
      "  -8.69956113e-04  4.54007855e-03  5.07708898e-04 -1.80360405e-02\n",
      "   1.80053620e-02  1.91277581e-04  4.80276842e-03 -4.46981125e-03\n",
      "  -4.38038993e-04  6.00624525e-03  1.27896631e-02  1.10664209e-03\n",
      "   6.42713694e-03 -1.10743984e-02 -8.58867581e-03  2.03314970e-02]\n",
      " [ 8.27464254e-03 -5.18206862e-03  7.21761297e-03 -1.88477618e-02\n",
      "   2.12877270e-03 -8.08930077e-03 -5.50489880e-04  1.81850084e-02\n",
      "   1.18209588e-02  8.52438741e-03 -9.55336789e-03 -1.19447663e-02\n",
      "   3.77512023e-04 -7.37502937e-03  9.92057615e-03  8.07109598e-03\n",
      "   3.93083637e-04 -6.99647270e-04 -1.74087011e-02  2.57643591e-03]\n",
      " [-1.11328832e-02 -7.73629711e-04 -2.20332119e-02  5.80456048e-03\n",
      "   1.22599665e-03  8.94903855e-04 -8.80181429e-03  6.52251077e-03\n",
      "  -2.17435794e-03 -2.08329118e-02 -1.61777884e-02 -1.85302295e-03\n",
      "   5.18266872e-03 -3.79664570e-03 -5.56878698e-04  1.19322806e-02\n",
      "  -1.33175898e-02  5.65711864e-04  1.34619774e-02 -4.05964135e-03]\n",
      " [-1.03853733e-02 -2.34877957e-03 -8.89240876e-03 -3.65794072e-03\n",
      "   3.39739958e-03 -9.33618863e-03  1.56881402e-02 -8.95410192e-03\n",
      "  -4.90995978e-03  9.82373393e-03 -6.10338424e-03  2.84421752e-03\n",
      "  -1.02952648e-02  1.03242835e-03 -5.19428180e-03 -9.48339636e-03\n",
      "  -1.44016823e-02 -1.45513434e-02 -8.62684890e-03 -7.20401399e-03]\n",
      " [-5.54648277e-03  1.19894731e-02 -5.98635751e-04  9.57446062e-03\n",
      "  -1.53724994e-02 -4.69339193e-03  6.63647169e-03  4.63506714e-03\n",
      "  -9.06110836e-03  4.56343164e-03  2.16003323e-03 -6.96583366e-03\n",
      "   9.43518569e-03  1.07590081e-02  5.69544857e-03 -7.54049533e-03\n",
      "  -6.84211004e-04 -2.21344639e-02  2.31432336e-03 -2.22769140e-02]]\n",
      "b2 =[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W3 =[[-0.00937116  0.0070847  -0.00155097  0.01563493 -0.00733401  0.0062031\n",
      "   0.01401611 -0.00801736 -0.01694634 -0.0009669 ]\n",
      " [-0.01227831 -0.00620646 -0.00662905  0.0123053  -0.00938444 -0.00588734\n",
      "   0.01390832 -0.01523196 -0.00126388 -0.00480367]\n",
      " [ 0.01964282  0.01091433 -0.00402545 -0.00993663  0.01097693  0.01512419\n",
      "  -0.0134073  -0.00477052 -0.00346355  0.02433431]\n",
      " [ 0.00698185 -0.0134197   0.00013366  0.01182902 -0.00220964  0.00472555\n",
      "  -0.00762    -0.00284465  0.00841318 -0.00247939]\n",
      " [-0.00527795  0.00958047  0.00791077 -0.0002355  -0.00251675 -0.01283686\n",
      "  -0.00329695 -0.00069242  0.00402515 -0.01745035]]\n",
      "b3 =[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W4 =[[-0.01032846 -0.0070032   0.01016569  0.01363593 -0.00527187]\n",
      " [-0.0003798   0.01548908 -0.02711274  0.00264206 -0.00328725]]\n",
      "b4 =[[0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Testing if the function works \n",
    "parameters = intialize_parameters([30,20,10,5,2])\n",
    "print(\"W1 =\" + str(parameters[\"W1\"]))\n",
    "print(\"b1 =\" + str(parameters[\"b1\"]))\n",
    "print(\"W2 =\" + str(parameters[\"W2\"]))\n",
    "print(\"b2 =\" + str(parameters[\"b2\"]))\n",
    "print(\"W3 =\" + str(parameters[\"W3\"]))\n",
    "print(\"b3 =\" + str(parameters[\"b3\"]))\n",
    "print(\"W4 =\" + str(parameters[\"W4\"]))\n",
    "print(\"b4 =\" + str(parameters[\"b4\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the sigmoid function \n",
    "def sigmoid(z):\n",
    "    \n",
    "    s = 1/(1+np.exp(-z))\n",
    "    cache = z\n",
    "    return s,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.88079708, 0.99908895]), array([2, 7]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test sigmoid function \n",
    "sigmoid(np.array(([2,7]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the relu function\n",
    "def relu(z):\n",
    "    \n",
    "    r = np.maximum(0,z)\n",
    "    cache = z\n",
    "    return r,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  0, 21]), [1, -1, 21])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing relu\n",
    "relu([1,-1,21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu Backward and Sigmoid Backward\n",
    "def relu_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear_forward\n",
    "def linear_forward(A, W, b):\n",
    "\n",
    "    Z = np.dot(W,A)+b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_activation_forward\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L layers forward propagation \n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A, cache = linear_activation_forward(A,parameters[\"W\" + str(l)],parameters[\"b\" + str(l)],activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A,parameters[\"W\" + str(L)],parameters[\"b\" + str(L)],activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Cost function\n",
    "\n",
    "def cost_function(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = (-1/m)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
    "\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_backward \n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward propagation\n",
    "\n",
    "def backward_propagation(AL, Y, caches):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL,current_cache,activation=\"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+2)],current_cache,activation=\"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update parameters \n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    for l in range(1,L+1):\n",
    "        parameters[\"W\"+str(l)]=parameters[\"W\" + str(l)]-learning_rate*grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\"+str(l)]=parameters[\"b\" + str(l)]-learning_rate*grads[\"db\" + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the size of the network \n",
    "layer_dims = [30,20,10,5,1] #5 Layer model with 3 hidden layers \n",
    "\n",
    "# Deep Learning network to classify frauds and normal\n",
    "layer_dims = [30,20,10,5,1] #5 Layer model with 3 hidden layers \n",
    "\n",
    "# Deep Learning network to classify frauds and normal\n",
    "def nn_model(X,Y,layer_dims,learning_rate=.0065, num_iterations=2500,print_cost=False):\n",
    "    costs = []\n",
    "    \n",
    "    #initialize parameters \n",
    "    parameters = intialize_parameters(layer_dims)\n",
    "    # for loop for iterations/epoch \n",
    "    for i in range(0,num_iterations):\n",
    "        #forward_propagation\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "        \n",
    "        #compute cost\n",
    "        cost = cost_function(AL, Y)\n",
    "        \n",
    "        #backward_propagation \n",
    "        grads = backward_propagation(AL, Y, caches)\n",
    "        \n",
    "        #update parameters\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 267718)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 267718)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.555161\n",
      "Cost after iteration 200: 0.454729\n",
      "Cost after iteration 300: 0.380449\n",
      "Cost after iteration 400: 0.324388\n",
      "Cost after iteration 500: 0.281178\n",
      "Cost after iteration 600: 0.247194\n",
      "Cost after iteration 700: 0.219967\n",
      "Cost after iteration 800: 0.197784\n",
      "Cost after iteration 900: 0.179441\n",
      "Cost after iteration 1000: 0.164068\n",
      "Cost after iteration 1100: 0.151032\n",
      "Cost after iteration 1200: 0.139860\n",
      "Cost after iteration 1300: 0.130196\n",
      "Cost after iteration 1400: 0.121764\n",
      "Cost after iteration 1500: 0.114352\n",
      "Cost after iteration 1600: 0.107791\n",
      "Cost after iteration 1700: 0.101947\n",
      "Cost after iteration 1800: 0.096713\n",
      "Cost after iteration 1900: 0.092000\n",
      "Cost after iteration 2000: 0.087736\n",
      "Cost after iteration 2100: 0.083862\n",
      "Cost after iteration 2200: 0.080329\n",
      "Cost after iteration 2300: 0.077093\n",
      "Cost after iteration 2400: 0.074120\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# running a model \n",
    "parameters = nn_model(X_train_set,Y_train_set,layer_dims,learning_rate=.0065,num_iterations = 2500, print_cost = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict Function\n",
    "def predict(X, y, parameters):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = forward_propagation(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9982780388319057\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(X_train_set, Y_train_set, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9982443820224721\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(X_test_flatten, Y_test_flatten, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9981275599765943\n"
     ]
    }
   ],
   "source": [
    "pred_dev = predict(X_dev_flatten, Y_dev_flatten, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
